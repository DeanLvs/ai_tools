from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import List, Optional
import os
import uuid
import httpx
import asyncio
import json
from typing import Optional
from datetime import datetime
from fastapi.responses import StreamingResponse
from book_yes_logger_config import logger
from fastapi.middleware.cors import CORSMiddleware
app = FastAPI(title="LobeChat Proxy with TTS & SadTalker")
# æ·»åŠ  CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],           # æˆ–è€… ["https://your.domain.com"]
    allow_credentials=True,
    allow_methods=["*"],           # å…è®¸ GETã€POSTã€OPTIONS ç­‰æ‰€æœ‰æ–¹æ³•
    allow_headers=["*"],           # å…è®¸æ‰€æœ‰å¤´éƒ¨
)
# === é…ç½® ===
LLM_RE_URL         = "http://localhost:6872/re"
TTS_URL            = "http://localhost:5018/process_tts"
TTS_RE_URL         = "http://localhost:5018/re"
SADTALKER_URL      = "http://localhost:5720/process_sadtalker"
SADTALKER_RE_URL   = "http://localhost:5720/re"

TTS_SAVE_DIR       = "./tts_output"
VIDEO_SAVE_DIR     = "./video_output"
TTS_ACCESS_PATH    = "/static/tts"
VIDEO_ACCESS_PATH  = "/static/video"

os.makedirs(TTS_SAVE_DIR, exist_ok=True)
os.makedirs(VIDEO_SAVE_DIR, exist_ok=True)

app.mount(TTS_ACCESS_PATH, StaticFiles(directory=TTS_SAVE_DIR), name="tts")
app.mount(VIDEO_ACCESS_PATH, StaticFiles(directory=VIDEO_SAVE_DIR), name="video")

LLM_JSON_URL    = "http://localhost:6872/chat_no_chat"         # éæµå¼
LLM_STREAM_URL  = "http://localhost:6872/chat_no_chat_stream"  # æµå¼


# === Pydantic å®šä¹‰ ===
# === Pydantic ===
class Message(BaseModel):
    id: Optional[str] = None
    role: Optional[str] = None
    content: Optional[str] = None

    class Config:
        extra = "allow"

class ChatRequest(BaseModel):
    model: Optional[str] = None
    stream: Optional[bool] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None

    messages: List[Message] = Field(default_factory=list)

    # åŸæ¥æ‰©å±•å­—æ®µ
    char_id: Optional[str] = None
    prompt_speech_path: Optional[str] = None
    image_path: Optional[str] = None

    # ä½ æ–°åŠ çš„æ‰€æœ‰å­—æ®µ
    user_id: Optional[str] = None
    user_rel: Optional[str] = None
    user_name: Optional[str] = None
    user_sex: Optional[str] = None
    user_role: Optional[str] = None

    char_time: Optional[str] = None
    char_name: Optional[str] = None
    char_sex: Optional[str] = None
    char_age: Optional[str] = None
    char_zhiye: Optional[str] = None
    char_changjing: Optional[str] = None
    char_des: Optional[str] = None

    class Config:
        extra = "allow"


# === é€šç”¨è°ƒç”¨ + é‡Šæ”¾æ˜¾å­˜ ===
async def call_and_release(post_url: str, release_url: str, payload: dict, save_dir: str, file_ext: str) -> Optional[str]:
    try:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(post_url, json=payload)
            resp.raise_for_status()
            data = resp.content
        filename = f"{uuid.uuid4().hex}.{file_ext}"
        out_path = os.path.join(save_dir, filename)
        with open(out_path, "wb") as f:
            f.write(data)
        asyncio.create_task(httpx.AsyncClient().get(release_url))
        return filename
    except Exception as e:
        app.logger = getattr(app, "logger", print)
        app.logger(f"Error calling {post_url}: {e}")
        return None


async def synthesize_tts(text: str, prompt_speech_path: Optional[str] = None) -> Optional[str]:
    # ä½¿ç”¨è¯·æ±‚æ–¹ä¼ å…¥çš„ prompt_speech_pathï¼Œå¦åˆ™å›é€€åˆ° assets/ref_voice.wav
    payload = {
        "text": text,
        "prompt_speech_path": prompt_speech_path or "assets/ref_voice.wav"
    }
    fn = await call_and_release(TTS_URL, TTS_RE_URL, payload, TTS_SAVE_DIR, "zip")
    return f"{TTS_ACCESS_PATH}/{fn}" if fn else None


async def synthesize_video(audio_path: str, image_path: Optional[str] = None) -> Optional[str]:
    # ä½¿ç”¨è¯·æ±‚æ–¹ä¼ å…¥çš„ image_pathï¼Œå¦åˆ™å›é€€åˆ° assets/ref_face.png
    payload = {
        "driven_audio": audio_path,
        "source_image": image_path or "assets/ref_face.png"
    }
    fn = await call_and_release(SADTALKER_URL, SADTALKER_RE_URL, payload, VIDEO_SAVE_DIR, "zip")
    return f"{VIDEO_ACCESS_PATH}/{fn}" if fn else None

# === èŠå¤©ä¸»æ¥å£ ===
@app.post("/chat/completions")
async def completions(request: Request, chat_req: ChatRequest):

    logger.info(f'receive {chat_req}')
    # === âœ¨ ç‰¹æ®ŠæŒ‡ä»¤ï¼šç³»ç»Ÿæ¶ˆæ¯ä¸­è¦æ±‚ç”Ÿæˆ 10 å­—ä»¥å†…æ ‡é¢˜ ===
    special = next(
        (m for m in chat_req.messages
         if m.role == "system" and "ä½ æ˜¯ä¸€åæ“…é•¿ä¼šè¯çš„åŠ©ç†" in m.content),
        None
    )

    def build_chunk(content: str, finish: bool = False):
        """ç”Ÿæˆç¬¦åˆ OpenAI ChatCompletion æµå¼è§„èŒƒçš„å­—å…¸"""
        if finish:
            delta = {}
            finish_reason = "stop"
        else:
            delta = {"content": content}
            finish_reason = None
        return {
            "id": "chatcmpl-local",
            "object": "chat.completion.chunk",
            "created": int(asyncio.get_event_loop().time()),
            "model": chat_req.model,
            "choices": [{
                "index": 0,
                "delta": delta,
                "finish_reason": finish_reason
            }]
        }

    if special:
        logger.info("å‘ç°å¯ç›´æ¥è¿”å›")
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        if chat_req.stream:  # è‹¥å‰ç«¯è¦æ±‚ SSE
            def fake_stream():
                yield f"data: {json.dumps(build_chunk(now_str), ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"

            return StreamingResponse(fake_stream(),
                                     media_type="text/event-stream")

    # === ç­›é€‰ messagesï¼Œå‰”é™¤ä¸éœ€è¦çš„è§’è‰² / é¦–æ¡ user ===
    original_messages = chat_req.messages[:]

    # åˆ é™¤æ‰€æœ‰ system æ¶ˆæ¯
    filtered_messages = [m for m in original_messages if m.role != "system"]

    # åˆ é™¤æœ€åä¸€æ¡ user æ¶ˆæ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    for i in range(len(filtered_messages) - 1, -1, -1):
        if filtered_messages[i].role == "user":
            del filtered_messages[i]
            break

    # æå– user_msgï¼ˆä»åŸå§‹æ¶ˆæ¯ä¸­æ‰¾æœ€åä¸€æ¡ userï¼‰
    try:
        user_msg = next(m.content for m in reversed(original_messages) if m.role == "user")
    except StopIteration:
        raise HTTPException(400, "No user message")

    llm_payload = {
        "char_id": chat_req.char_id or "default-char",
        "user_id": chat_req.user_id,
        "user_rel": chat_req.user_rel,
        "user_name": chat_req.user_name,
        "user_sex": chat_req.user_sex,
        "user_role": chat_req.user_role,
        "char_time": chat_req.char_time,
        "char_name": chat_req.char_name,
        "char_sex": chat_req.char_sex,
        "char_age": chat_req.char_age,
        "char_zhiye": chat_req.char_zhiye,
        "char_changjing": chat_req.char_changjing,
        "char_des": chat_req.char_des,
        "prompt_speech_path": chat_req.prompt_speech_path,
        "image_path": chat_req.image_path,
        "message": user_msg,
        "history_turns": 0,
        # messages éƒ¨åˆ†ï¼Œä½ å¯ä»¥æŒ‰éœ€å¤„ç†ï¼›è¿™é‡Œç›´æ¥ä¼ æ•´ä¸ªåˆ—è¡¨
        "history_external": [m.dict() for m in filtered_messages],
    }


    # --------------------- éæµå¼ ---------------------
    if not chat_req.stream:
        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(LLM_JSON_URL, json=llm_payload)
            resp.raise_for_status()
        reply = resp.json()["answer"]
        # reply = "mockæ•°æ®"
        # ï¼ˆå¦‚éœ€ TTS/SadTalkerï¼Œå¯åœ¨æ­¤åŒæ­¥è°ƒç”¨ï¼Œå†æ‹¼ replyï¼‰
        return {
            "id": "chatcmpl-local",
            "object": "chat.completion",
            "created": int(asyncio.get_event_loop().time()),
            "model": chat_req.model,
            "choices": [{
                "index": 0,
                "message": {"role": "assistant", "content": reply},
                "finish_reason": "stop"
            }],
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
    async def event_stream():

        async with httpx.AsyncClient(timeout=120) as client:
            resp = await client.post(LLM_JSON_URL, json=llm_payload)
            resp.raise_for_status()
        reply = resp.json()["answer"]

        # â€”â€” ä¸€å®šè¦å…ˆæŠŠ reply èµ‹ç»™ answer_acc â€”â€”
        answer_acc = reply

        # â€”â€” æŠŠæ•´æ®µ reply ä½œä¸ºç¬¬ä¸€ä¸ª delta æ¨é€ â€”â€”
        yield f"data: {json.dumps(build_chunk(reply), ensure_ascii=False)}\n\n"
        # â€”â€” 2) ç­‰å¾…å¤šåª’ä½“ç”Ÿæˆï¼ˆåŒæ­¥é˜»å¡ï¼ŒSSE ä»ä¿æŒæ‰“å¼€ï¼‰ â€”â€” #
        links_text = ""
        tts_url: Optional[str] = None
        if chat_req.prompt_speech_path:
            tts_url = await synthesize_tts(answer_acc, chat_req.prompt_speech_path)
            if tts_url:
                links_text += f"\n\nğŸ§ [è¯­éŸ³æ’­æ”¾]({tts_url})"

        if chat_req.image_path:
            audio_input = tts_url or "assets/ref_voice.wav"
            video_url = await synthesize_video(audio_input, chat_req.image_path)
            if video_url:
                links_text += f"\n\nğŸ¬ [æŸ¥çœ‹åŠ¨ç”»]({video_url})"

        if links_text:
            # æŠŠå¤šåª’ä½“é“¾æ¥å½“ä½œæ–°çš„ delta å†æ¨ä¸€æ¬¡
            yield f"data: {json.dumps(build_chunk(links_text))}\n\n"
            answer_acc += links_text  # å¦‚éœ€å†™å†å²ï¼Œå¯ç”¨å®ƒ

        # â€”â€” 3) å‘é€ stop-chunk + [DONE] â€”â€” #
        yield f"data: {json.dumps(build_chunk('', finish=True))}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(event_stream(),
                             media_type="text/event-stream")


# === å¯åŠ¨æ–¹å¼ ===
# pip install fastapi uvicorn httpx
# uvicorn proxy_server:app --host 0.0.0.0 --port 8000
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("proxy_server:app", host="0.0.0.0", port=5008, workers=1)